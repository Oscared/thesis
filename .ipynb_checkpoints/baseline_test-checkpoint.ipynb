{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for testing baseline methods\n",
    "### Supervised: SVM, RF, XGBOOST\n",
    "### Semi-supervised: TSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import visdom\n",
    "import utils\n",
    "import seaborn as sns\n",
    "from datasets import get_patch_data, get_pixel_idx, HyperX_patches\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Data/ieee_supplement/Hyperspectral_Grids/Salinas/'\n",
    "\n",
    "label_values = [\"Undefined\", \"Brocoli_green_weeds_1\", \"Brocoli_green_weeds_2\", \"Fallow\",\n",
    "                        \"Fallow_rough_plow\", \"Fallow_smooth\", \"Stubble\",\n",
    "                        \"Celery\", \"Grapes_untrained\", \"Soil_vinyard_develop\",\n",
    "                        \"Corn_senesced_green_weeds\", \"Lettuce_romaine_4wk\", \"Lettuce_romaine_5wk\",\n",
    "                        \"Lettuce_romaine_6wk\", \"Lettuce_romaine_7wk\", \"Vinyard_untrained\",\n",
    "                        \"Vinyard_vertical_trellis\"]\n",
    "\n",
    "palette = None\n",
    "rgb_bands = (43, 21, 11)\n",
    "\n",
    "if palette is None:\n",
    "    # Generate color palette\n",
    "    palette = {0: (0, 0, 0)}\n",
    "    for k, color in enumerate(sns.color_palette(\"hls\", len(label_values) - 1)):\n",
    "        palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(x):\n",
    "    return utils.convert_to_color_(x, palette=palette)\n",
    "def convert_from_color(x):\n",
    "    return utils.convert_from_color_(x, palette=invert_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 4\n",
    "\n",
    "train_img, train_gt, test_img, test_gt, label_values, ignored_labels, rgb_bands, palette = get_patch_data('Salinas', 5, target_folder=data_path, fold=fold)\n",
    "\n",
    "if palette is None:\n",
    "    # Generate color palette\n",
    "    palette = {0: (0, 0, 0)}\n",
    "    for k, color in enumerate(sns.color_palette(\"hls\", len(label_values) - 1)):\n",
    "        palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(x):\n",
    "    return utils.convert_to_color_(x, palette=palette)\n",
    "def convert_from_color(x):\n",
    "    return utils.convert_from_color_(x, palette=invert_palette)\n",
    "\n",
    "n_bands = train_img.shape[-1]\n",
    "n_classes = len(label_values) - len(ignored_labels)\n",
    "\n",
    "idx_sup, idx_val, idx_unsup = get_pixel_idx(train_img, train_gt, ignored_labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {'patch_size': 5, 'dataset': 'Salinas', 'ignored_labels': ignored_labels, \n",
    "               'flip_augmentation': False, 'radiation_augmentation': False, 'mixture_augmentation': False,\n",
    "              'center_pixel': True, 'supervision': 'full', 'pca_augmentation': False, 'pca_strength': 1, 'cutout_spatial': False,\n",
    "              'cutout_spectral': False, 'augmentation_magnitude': 1, 'spatial_combinations': False, 'spectral_mean': False,\n",
    "              'moving_average': False}\n",
    "\n",
    "val_dataset = HyperX_patches(train_img, train_gt, idx_val, labeled=True, **hyperparams)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=10)\n",
    "\n",
    "train_labeled_dataset = HyperX_patches(train_img, train_gt, idx_sup, labeled=True, **hyperparams)\n",
    "train_labeled_loader = data.DataLoader(train_labeled_dataset, batch_size=len(idx_sup),\n",
    "                                       #pin_memory=True, num_workers=5,\n",
    "                                       shuffle=True, drop_last=True)\n",
    "\n",
    "unlabeled_ratio = math.ceil(len(idx_unsup)/len(idx_sup))\n",
    "\n",
    "train_unlabeled_dataset = HyperX_patches(train_img, train_gt, idx_unsup, labeled=False, **hyperparams)\n",
    "train_unlabeled_loader = data.DataLoader(train_unlabeled_dataset, batch_size=len(idx_unsup),\n",
    "                                       #pin_memory=True, num_workers=5,\n",
    "                                       shuffle=True, drop_last=True)\n",
    "amount_labeled = idx_sup.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move data to simple list with pixel form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "X = np.zeros((len(idx_sup), n_bands))\n",
    "Y = np.zeros(len(idx_sup))\n",
    "for p, x, y in idx_sup:\n",
    "    X[i, :] = train_img[p,x,y]\n",
    "    Y[i] = train_gt[p,x,y] - 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "X_un = np.zeros((len(idx_unsup), n_bands))\n",
    "for p, x, y in idx_unsup:\n",
    "    X_un[i, :] = train_img[p,x,y]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_GRID_PARAMS = [{'kernel': ['rbf'], 'gamma': [5e-1, 2e-1], 'C': [1000, 1500]}]\n",
    "\n",
    "class_weight = None\n",
    "clf = SVM.SVC(class_weight=class_weight, kernel='rbf', gamma=0.5, C=1000)\n",
    "#clf = sklearn.model_selection.GridSearchCV(clf, SVM_GRID_PARAMS, verbose=5, n_jobs=4)\n",
    "clf.fit(X, Y)\n",
    "#print('SVMs best params: {} '.format(clf.best_params_))\n",
    "#save_model(clf, 'SVM', 'Salinas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(test_img.reshape(-1, n_bands))\n",
    "prediction = prediction.reshape(test_img.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_GRID_PARAMS = [{'max_depth': [32, 256, 512], 'n_estimators': [100, 200, 400]}]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators = 200, min_samples_split=2, \\\n",
    "                    max_features=10, max_depth=10)\n",
    "#clf = sklearn.model_selection.GridSearchCV(clf, RF_GRID_PARAMS, verbose=5, n_jobs=4)\n",
    "clf.fit(X, Y)\n",
    "#print('RFs best params: {} '.format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(test_img.reshape(-1, n_bands))\n",
    "prediction = prediction.reshape(test_img.shape[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=200, max_depth=10, objective='multi::softmax', num_class=n_classes)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "prediction = clf.predict(test_img.reshape(-1, n_bands))\n",
    "prediction = prediction.reshape(test_img.shape[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_labeled = len(X)\n",
    "nb_unlabeled = len(X_un)\n",
    "nb_samples = nb_labeled + nb_unlabeled\n",
    "\n",
    "n_bands = X.shape[-1]\n",
    "\n",
    "X_t = np.concatenate((X, X_un))\n",
    "Y_t = np.concatenate((Y, -1*np.ones(nb_unlabeled)))\n",
    "\n",
    "w = np.random.uniform(-0.1, 0.1, size=n_bands)\n",
    "eta_labeled = np.random.uniform(0.0, 0.1, size=nb_labeled)\n",
    "eta_unlabeled = np.random.uniform(0.0, 0.1, size=nb_unlabeled)\n",
    "y_unlabeled = np.random.uniform(-1.0, 1.0, size=nb_unlabeled)\n",
    "b = np.random.uniform(-0.1, 0.1, size=1)\n",
    "\n",
    "C_labeled = 1.0\n",
    "C_unlabeled = 10.0\n",
    "\n",
    "theta0 = np.hstack((w, eta_labeled, eta_unlabeled, y_unlabeled, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_target(theta, Xd, Yd):\n",
    "    wt = theta[0:n_bands].reshape((n_bands, 1))\n",
    "    s_eta_labeled = np.sum(theta[n_bands:n_bands+nb_labeled])\n",
    "    s_eta_unlabeled = np.sum(theta[n_bands+nb_labeled:n_bands+nb_samples])\n",
    "    return (C_labeled*s_eta_labeled) + (C_unlabeled*s_eta_unlabeled) + (0.5*np.dot(wt.T, wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled and unlabeled constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_constraint(theta, Xd, Yd, idx):\n",
    "    wt = theta[0:n_bands].reshape((n_bands, 1))\n",
    "    c = Yd[idx] * (np.dot(Xd[idx], wt) + theta[-1]) + \\\n",
    "    theta[n_bands:n_bands+nb_labeled][idx] - 1.0\n",
    "    return (c>=0)[0]\n",
    "\n",
    "def unlabeled_constraint(theta, Xd, idx):\n",
    "    wt = theta[0:n_bands].reshape((n_bands, 1))\n",
    "    c = theta[n_bands+nb_labeled:n_bands+nb_samples+nb_unlabeled][idx-nb_samples+nb_unlabeled]*\\\n",
    "    (np.dot(Xd[idx],wt) + theta[-1]) + \\\n",
    "    theta[n_bands+nb_labeled:n_bands+nb_samples][idx-nb_samples+nb_unlabeled] - 1.0\n",
    "    return (c>=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slack constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_labeled_constraint(theta, idx):\n",
    "    return theta[n_bands:n_bands+nb_labeled][idx] >= 0\n",
    "\n",
    "def eta_unlabeled_constraint(theta, idx):\n",
    "    return theta[n_bands+nb_labeled:n_bands+nb_samples][idx-nb_samples+nb_unlabeled] >= 0\n",
    "\n",
    "def y_constraint(theta, idx):\n",
    "    return np.power(theta[n_bands+nb_samples:n_bands+nb_samples+nb_unlabeled][idx], 2) == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM constraints for scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_constraints = []\n",
    "\n",
    "for i in range(nb_labeled):\n",
    "    svm_constraints.append({\n",
    "        'type': 'ineq',\n",
    "        'fun': labeled_constraint,\n",
    "        'args': (X, Y, i)\n",
    "    })\n",
    "    svm_constraints.append({\n",
    "        'type': 'ineq',\n",
    "        'fun': eta_labeled_constraint,\n",
    "        'args': (i, )\n",
    "    })\n",
    "    \n",
    "for i in range(nb_labeled, nb_samples):\n",
    "    svm_constraints.append({\n",
    "        'type': 'ineq',\n",
    "        'fun': unlabeled_constraint,\n",
    "        'args': (X, i)\n",
    "    })\n",
    "    svm_constraints.append({\n",
    "        'type': 'ineq',\n",
    "        'fun': eta_unlabeled_constraint,\n",
    "        'args': (i, )\n",
    "    })\n",
    "    \n",
    "for i in range(nb_unlabeled):\n",
    "    svm_constraints.append({\n",
    "        'type': 'eq',\n",
    "        'fun': y_constraint,\n",
    "        'args': (i,)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run scipy to optimze this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=minimize(fun=svm_target, \n",
    "               x0=theta0,\n",
    "               constraints=svm_constraints,\n",
    "               args=(X_t,Y_t),\n",
    "               method='SLSQP',\n",
    "               tol=0.0001,\n",
    "               options={'maxiter': 1000}\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to perform multiclass classification we need to create n_classes amount of TSVM's and train each on classifying if a sample belongs to one class or not. These will then need to perform a winner-takes-it-all approach to choose which class a sample belongs to. We also need to transform the linear kernel to a gaussian kernel. It is also benefitial to instead move to Lagrangian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsvm_target(theta, Xd, Yd):\n",
    "    wt = theta[0:n_bands].reshape((n_bands, 1))\n",
    "    s_eta_labeled = np.sum(theta[n_bands:n_bands+nb_labeled])\n",
    "    s_eta_unlabeled = np.sum(theta[n_bands+nb_labeled:n_bands+nb_samples])\n",
    "    return (C_labeled*s_eta_labeled) + (C_unlabeled*s_eta_unlabeled) + (0.5*np.dot(wt.T, wt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSVM implementation from Bruzzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import qns3vm\n",
    "import scipy\n",
    "import sklearn.svm as SVM \n",
    "import numpy as np\n",
    "\n",
    "qns3vm = importlib.reload(qns3vm)\n",
    "\n",
    "from qns3vm import QN_S3VM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = 10\n",
    "\n",
    "C_labeled = 1.0\n",
    "C_unlabeled = np.zeros(G)\n",
    "C_unlabeled[0] = C_labeled/(10*G)\n",
    "C_max = 10.0\n",
    "\n",
    "sigma = 1.0\n",
    "Np = 0\n",
    "Nm = 0\n",
    "A = np.zeros(n_classes)\n",
    "\n",
    "rand_generator = random.Random()\n",
    "\n",
    "CLF = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    idx_class = Y == i\n",
    "    idx_rest = Y != i\n",
    "    X_train = np.concatenate((X[idx_class], X[idx_rest]))\n",
    "    Y_train = np.concatenate((np.ones(len(X[idx_class])).astype(int), -np.ones(len(X[idx_rest])).astype(int)))\n",
    "    # Classifier is a transductive SVM\n",
    "    #CLF.append(QN_S3VM(X_train.tolist(), Y_train.tolist(), X_un.tolist(), rand_generator, lam=C_labeled, lamU=C_unlabeled[0], kernel_type='RBF'))\n",
    "    #CLF[i].train()\n",
    "    \n",
    "    # Classifier is a standard SVM\n",
    "    CLF.append(SVM.SVC(kernel='rbf', gamma=0.5, C=C_labeled)) #C=1000 other option\n",
    "    CLF[i].fit(X_train,Y_train)\n",
    "    \n",
    "    support = CLF[i].n_support_\n",
    "    \n",
    "    Np = support[1]\n",
    "    Nm = support[0]\n",
    "    \n",
    "    A[i] = np.min((Np,Nm))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in range(G):\n",
    "    for i in range(n_classes):\n",
    "        #Find A transductive samples\n",
    "        values = CLF[i].decision_function(X_un)\n",
    "        values = np.asarray(values)\n",
    "        \n",
    "        if np.max(values)>0 and np.min(values)<0:\n",
    "            #Extract values of positive unlabeled samples\n",
    "            values_p = np.abs(1 - values[values>0])\n",
    "            idx_p = values_p.argsort()[:int(A[i])]\n",
    "            #Threshold\n",
    "            D_p = np.sum(values[idx_p])/A[i]\n",
    "            Th_p = D_p*np.max(np.abs(values[idx_p]))\n",
    "            #Trim candidate set\n",
    "            N_p = len(values[idx_p][np.abs(values[idx_p])>=Th_p])\n",
    "        \n",
    "        \n",
    "            values_m = np.abs(-1 - values[values<0])\n",
    "            idx_m = values_m.argsort()[:int(A[i])]\n",
    "            #Threshold\n",
    "            D_m = np.sum(values[idx_m])/A[i]\n",
    "            Th_m = D_m*np.max(np.abs(values[idx_m]))\n",
    "            #Trim candidate set\n",
    "            N_m = len(values[idx_m][np.abs(values[idx_m])>=Th_m])\n",
    "            \n",
    "            N = np.min((N_p, N_m))\n",
    "        \n",
    "            if N == N_p:\n",
    "                X_un_p = X_un[idx_p]\n",
    "                X_un_m = X_un[idx_m[:N]]\n",
    "                del_idx = np.concatenate((idx_p, idx_m[:N]))\n",
    "            elif N==N_m:\n",
    "                X_un_m = X_un[idx_m]\n",
    "                X_un_p = X_un[idx_p[:N]]\n",
    "                del_idx = np.concatenate((idx_m, idx_p[:N]))\n",
    "               \n",
    "            #Update datasets               \n",
    "            idx_class = Y == i\n",
    "            idx_rest = Y != i\n",
    "            X_train = np.concatenate((X[idx_class], X[idx_rest]))\n",
    "            Y_train = np.concatenate((np.ones(len(X[idx_class])), -np.ones(len(X[idx_rest]))))\n",
    "            \n",
    "            np.append(X_train, (X_un_m, X_un_p))\n",
    "            np.append(Y_train, (-np.ones(len(X_un_m)), np.ones(len(X_un_p))))\n",
    "            X_un = np.delete(X_un, del_idx,0)\n",
    "               \n",
    "            #Update weight factor        \n",
    "            #C[i] = (C_max - C[0])/G^2*g^2 + C[0]\n",
    "        \n",
    "            #Retrain the TSVM\n",
    "            CLF[i].fit(X_train, Y_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_values = []\n",
    "for i in range(n_classes):\n",
    "    pred_values.append(CLF[i].decision_function(test_img.reshape(-1, n_bands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = np.asarray(pred_values)\n",
    "prediction = np.argmax(predicted_values, axis=0)\n",
    "prediction = prediction.reshape(test_img.shape[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with the QN-S3VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "warnings.filterwarnings('ignore', message='the matrix subclass is not the recommended way')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No go classes: []\n"
     ]
    }
   ],
   "source": [
    "G = 10\n",
    "\n",
    "C_labeled = 1.0\n",
    "C_max = 10.0\n",
    "C = np.zeros((n_classes, G))\n",
    "C[:,0] = C_labeled/(10*G)\n",
    "\n",
    "sigma = 1.0\n",
    "Np = 0\n",
    "Nm = 0\n",
    "A = np.zeros(n_classes)\n",
    "\n",
    "rand_generator = random.Random()\n",
    "\n",
    "CLF = []\n",
    "\n",
    "no_go = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    idx_class = Y == i\n",
    "    idx_rest = Y != i\n",
    "    X_train = np.concatenate((X[idx_class], X[idx_rest]))\n",
    "    Y_train = np.concatenate((np.ones(len(X[idx_class])).astype(int), -np.ones(len(X[idx_rest])).astype(int)))\n",
    "    # Classifier is a transductive SVM\n",
    "    CLF.append(SVM.SVC(kernel='rbf', gamma=0.5, C=C_labeled)) #C=1000 other option\n",
    "    if np.max(idx_class) == 0:\n",
    "        no_go.append(i)\n",
    "        break\n",
    "    CLF[i].fit(X_train,Y_train)\n",
    "\n",
    "    support = CLF[i].n_support_\n",
    "\n",
    "    Np = support[1]\n",
    "    Nm = support[0]\n",
    "\n",
    "    A[i] = np.min((Np,Nm))\n",
    "\n",
    "print('No go classes: ' + str(no_go))\n",
    "yes_go = np.delete(range(n_classes), no_go)   \n",
    "labeled = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yes_go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-58e4066a5dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCLF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myes_go\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0midx_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0midx_rest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yes_go' is not defined"
     ]
    }
   ],
   "source": [
    "CLF={}\n",
    "\n",
    "for i in yes_go:\n",
    "    idx_class = Y == i\n",
    "    idx_rest = Y != i\n",
    "    X_train = np.concatenate((X[idx_class], X[idx_rest]))\n",
    "    Y_train = np.concatenate((np.ones(len(X[idx_class])).astype(int), -np.ones(len(X[idx_rest])).astype(int)))\n",
    "    # Classifier is a transductive SVM\n",
    "    CLF[i] = (QN_S3VM(X_train.tolist(), Y_train.tolist(), X_un.tolist(), rand_generator, lam=C_labeled, lamU=10.0, kernel_type='RBF'))\n",
    "    CLF[i].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {}\n",
    "dict[1] = [1,2,3]\n",
    "dict[2] = 'hi'\n",
    "len(dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in yes_go:\n",
    "    X_un_run = X_un\n",
    "    for g in range(G):\n",
    "        if g == 0:\n",
    "            print('Running class: ' + str(i) + '. Time: ' + str(g))\n",
    "            #Find A transductive samples\n",
    "            values = CLF[i].decision_function(X_un_run)\n",
    "            values = np.asarray(values)\n",
    "        else:\n",
    "            print('Running class: ' + str(i) + '. Time: ' + str(g))\n",
    "            #Find A transductive samples\n",
    "            values = np.zeros(X_un_run.shape[0])\n",
    "            for idx, x in np.ndenumerate(X_un_run):\n",
    "                values[idx] = CLF[-1].predict(x)\n",
    "\n",
    "        if np.max(values)>0 and np.min(values)<0:\n",
    "            #Extract values of positive unlabeled samples\n",
    "            values_p = np.abs(1 - values[values>0])\n",
    "            idx_p = values_p.argsort()[:int(A[i])]\n",
    "            #Threshold\n",
    "            D_p = np.sum(values[idx_p])/A[i]\n",
    "            Th_p = D_p*np.max(np.abs(values[idx_p]))\n",
    "            #Trim candidate set\n",
    "            N_p = len(values[idx_p][np.abs(values[idx_p])>=Th_p])\n",
    "\n",
    "\n",
    "            values_m = np.abs(-1 - values[values<0])\n",
    "            idx_m = values_m.argsort()[:int(A[i])]\n",
    "            #Threshold\n",
    "            D_m = np.sum(values[idx_m])/A[i]\n",
    "            Th_m = D_m*np.max(np.abs(values[idx_m]))\n",
    "            #Trim candidate set\n",
    "            N_m = len(values[idx_m][np.abs(values[idx_m])>=Th_m])\n",
    "\n",
    "            N = np.min((N_p, N_m))\n",
    "\n",
    "            if N == N_p:\n",
    "                X_un_p = X_un_run[idx_p]\n",
    "                X_un_m = X_un_run[idx_m[:N]]\n",
    "                del_idx = np.concatenate((idx_p, idx_m[:N]))\n",
    "            elif N==N_m:\n",
    "                X_un_m = X_un_run[idx_m]\n",
    "                X_un_p = X_un_run[idx_p[:N]]\n",
    "                del_idx = np.concatenate((idx_m, idx_p[:N]))\n",
    "\n",
    "            #Update datasets\n",
    "            idx_class = Y == i\n",
    "            idx_rest = Y != i\n",
    "            X_train = np.concatenate((X[idx_class], X[idx_rest]))\n",
    "            Y_train = np.concatenate((np.ones(len(X[idx_class])).astype(int), -np.ones(len(X[idx_rest])).astype(int)))\n",
    "\n",
    "            X_train = np.append(X_train, np.concatenate((X_un_m, X_un_p)), axis=0)\n",
    "            X_un_run = np.delete(X_un_run, del_idx,0)\n",
    "\n",
    "            #Update weight factor\n",
    "            C[i,g] = ((C_max - C[i,0])/(G^2))*(g^2) + C[i,0]\n",
    "\n",
    "            #Retrain the TSVM\n",
    "            CLF.append(QN_S3VM(X_train.tolist()[:labeled], Y_train.tolist(), X_train.tolist()[labeled:], rand_generator, lam=C_labeled, lamU=C_unlabeled[i], kernel_type='RBF'))\n",
    "            CLF[-1].train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_un_run.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_results = utils.metrics(prediction, test_gt, ignored_labels=ignored_labels, n_classes=n_classes)\n",
    "\n",
    "mask = np.zeros(test_gt.shape, dtype='bool')\n",
    "for l in ignored_labels:\n",
    "    mask[test_gt == l] = True\n",
    "prediction += 1\n",
    "prediction[mask] = 0\n",
    "\n",
    "color_prediction = convert_to_color(prediction)\n",
    "utils.display_predictions(color_prediction, vis, gt=convert_to_color(test_gt), caption=\"Prediction vs. test ground truth\")\n",
    "\n",
    "utils.show_results(run_results, vis, label_values=label_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(run_results['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(results)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
