{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import visdom\n",
    "from datasets import get_dataset, HyperX\n",
    "import utils\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data\n",
    "#from torchsummary import summary\n",
    "#from torch.utils.tensorboard  import SummaryWriter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "#import joblib\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import models\n",
    "\n",
    "vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3467 samples selected (over 54129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3506: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/oscar/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:207: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "img, gt, label_values, ignored_labels, rgb_bands, palette = get_dataset('Salinas', target_folder='/home/oscar/Desktop/Exjobb/Data/')\n",
    "\n",
    "N_CLASSES = len(label_values)\n",
    "N_BANDS = img.shape[-1]\n",
    "IGNORED_LABELS = ignored_labels\n",
    "PATCH_SIZE = 5\n",
    "SAMPLING_MODE = 'disjoint'\n",
    "SAMPLING_PERCENTAGE = 0.05\n",
    "BATCH_SIZE = 10\n",
    "UNLABELED_RATIO = 7\n",
    "EPOCHS = 10\n",
    "\n",
    "hyperparams = {'patch_size': PATCH_SIZE, 'dataset': 'Salinas', 'ignored_labels': IGNORED_LABELS, \n",
    "               'flip_augmentation': True, 'radiation_augmentation': False, 'mixture_augmentation': False,\n",
    "              'center_pixel': True, 'supervision': 'full'}\n",
    "\n",
    "if palette is None:\n",
    "    # Generate color palette\n",
    "    palette = {0: (0, 0, 0)}\n",
    "    for k, color in enumerate(sns.color_palette(\"hls\", len(label_values) - 1)):\n",
    "        palette[k + 1] = tuple(np.asarray(255 * np.array(color), dtype='uint8'))\n",
    "invert_palette = {v: k for k, v in palette.items()}\n",
    "\n",
    "def convert_to_color(x):\n",
    "    return utils.convert_to_color_(x, palette=palette)\n",
    "def convert_from_color(x):\n",
    "    return utils.convert_from_color_(x, palette=invert_palette)\n",
    "\n",
    "train_gt, test_gt = utils.sample_gt(gt, SAMPLING_PERCENTAGE,\n",
    "                                    mode=SAMPLING_MODE)\n",
    "print(\"{} samples selected (over {})\".format(np.count_nonzero(train_gt), np.count_nonzero(gt)))\n",
    "\n",
    "model = models.HamidaEtAl(N_BANDS, N_CLASSES, patch_size=PATCH_SIZE)\n",
    "\n",
    "train_gt, val_gt = utils.sample_gt(train_gt, 0.95, mode=SAMPLING_MODE)\n",
    "\n",
    "val_dataset = HyperX(img, val_gt, labeled=True, **hyperparams)\n",
    "val_loader = data.DataLoader(val_dataset,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "\n",
    "train_labeled_gt, train_unlabeled_gt = utils.sample_gt(train_gt, 1/(UNLABELED_RATIO + 1),\n",
    "                                                       mode=SAMPLING_MODE)\n",
    "amount_labeled = np.count_nonzero(train_labeled_gt)\n",
    "\n",
    "train_labeled_dataset = HyperX(img, train_labeled_gt, labeled=True, **hyperparams)\n",
    "train_labeled_loader = data.DataLoader(train_labeled_dataset, batch_size=BATCH_SIZE,\n",
    "                                       #pin_memory=True,\n",
    "                                       shuffle=True, drop_last=True)\n",
    "\n",
    "train_unlabeled_dataset = HyperX(img, train_unlabeled_gt, labeled=False, **hyperparams)\n",
    "train_unlabeled_loader = data.DataLoader(train_unlabeled_dataset,\n",
    "                                         batch_size=BATCH_SIZE*UNLABELED_RATIO,\n",
    "                                         #pin_memory=True,\n",
    "                                         shuffle=True, drop_last=True)\n",
    "\n",
    "iterations = amount_labeled // BATCH_SIZE\n",
    "total_steps = iterations * EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000001"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0. 100. 170. 104.  57. 130. 183. 165. 492. 294. 148.  52. 104. 784.\n",
      "  57. 317.  82.]\n",
      "[    0.  2009.  3726.  1976.  1394.  2678.  3959.  3579. 11271.  6203.\n",
      "  3278.  1068.  1927.   916.  1070.  7268.  1807.]\n"
     ]
    }
   ],
   "source": [
    "weights = np.zeros(N_CLASSES)\n",
    "frequencies = np.zeros(N_CLASSES)\n",
    "train_freq = np.zeros(N_CLASSES)\n",
    "\n",
    "for c in range(0, N_CLASSES):\n",
    "    if c in IGNORED_LABELS:\n",
    "        continue\n",
    "    frequencies[c] = np.count_nonzero(gt == c)\n",
    "    train_freq[c] = np.count_nonzero(train_gt == c)\n",
    "\n",
    "print(train_freq)\n",
    "print(frequencies)\n",
    "    \n",
    "# Normalize the pixel counts to obtain frequencies\n",
    "frequencies /= np.sum(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04329472186812984"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(frequencies[np.nonzero(frequencies)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.16650075, 0.62895867, 1.18598178, 1.68113343,\n",
       "       0.87509335, 0.59194241, 0.65479184, 0.20792299, 0.37780106,\n",
       "       0.71491763, 2.19428839, 1.21613908, 2.55840611, 2.19018692,\n",
       "       0.32244084, 1.29690094])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain the median on non-zero frequencies\n",
    "median = np.median(frequencies[np.nonzero(frequencies)])\n",
    "weights = median / frequencies\n",
    "weights[frequencies == 0] = 0.\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.45594823, 0.24584004, 0.46356275, 0.65710187,\n",
       "       0.3420463 , 0.23137156, 0.25593741, 0.08127052, 0.14767048,\n",
       "       0.27943868, 0.8576779 , 0.47535029, 1.        , 0.85607477,\n",
       "       0.12603192, 0.50691754])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights/(np.max(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data = enumerate(zip(train_labeled_loader, train_unlabeled_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx, (data_l, data_u) = next(iter_data)\n",
    "input_l, target_l = data_l\n",
    "input_w, input_s = data_u\n",
    "\n",
    "inputs = torch.cat((input_l, input_w, input_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9215)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(input_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "logits=model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.9774, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_balance = utils.compute_imf_weights(train_gt, N_CLASSES, IGNORED_LABELS)\n",
    "weights = torch.from_numpy(weights_balance)\n",
    "weights = weights.to(torch.float32)\n",
    "\n",
    "loss_labeled = nn.CrossEntropyLoss(weight=weights)\n",
    "loss_unlabeled = nn.CrossEntropyLoss(weight=weights, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.CrossEntropyLoss"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_l = logits[0:BATCH_SIZE]\n",
    "logits_w, logits_s = logits[BATCH_SIZE:].chunk(2)\n",
    "\n",
    "sup_loss = loss_labeled(logits_l, target_l)\n",
    "\n",
    "psuedo_label = torch.softmax(logits_w.detach_(), dim=-1)\n",
    "max_probs, psuedo_target = torch.max(psuedo_label, dim=-1)\n",
    "mask = max_probs.ge(0.95).float()\n",
    "\n",
    "unsup_loss = (loss_unlabeled(logits_s, psuedo_target)*mask).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_probs[psuedo_target==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = sup_loss + unsup_loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.7449, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight tensor(-1.2744) tensor(1.0164)\n",
      "conv1.bias tensor(-1.7973) tensor(0.0537)\n",
      "pool1.weight tensor(-0.6922) tensor(0.7958)\n",
      "pool1.bias tensor(-2.9161) tensor(3.3856)\n",
      "conv2.weight tensor(-0.3681) tensor(0.4515)\n",
      "conv2.bias tensor(-0.6057) tensor(0.1901)\n",
      "pool2.weight tensor(-0.9111) tensor(0.6887)\n",
      "pool2.bias tensor(-0.3551) tensor(0.3516)\n",
      "conv3.weight tensor(-1.3570) tensor(0.9937)\n",
      "conv3.bias tensor(-0.2725) tensor(0.1747)\n",
      "conv4.weight tensor(-3.0408) tensor(0.5302)\n",
      "conv4.bias tensor(-0.5896) tensor(0.0875)\n",
      "fc.weight tensor(-2.2576) tensor(1.7449)\n",
      "fc.bias tensor(-0.0514) tensor(0.0661)\n"
     ]
    }
   ],
   "source": [
    "model_test = models.HamidaEtAl(N_BANDS, N_CLASSES, patch_size=PATCH_SIZE)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, torch.min(param.data), torch.max(param.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/home/oscar/Desktop/Exjobb/thesis/checkpoints/hamida_et_al/Salinas/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "max_logits = np.zeros(BATCH_SIZE)\n",
    "\n",
    "for idx, (data_x, data_u) in enumerate(zip(train_labeled_loader, train_unlabeled_loader)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    input_x, target_x = data_x\n",
    "    input_u_w, input_u_s = data_u\n",
    "    \n",
    "    inputs = torch.cat(input_x, input_u_w, input_u_s)\n",
    "    \n",
    "    logits = model(input)\n",
    "    max_logits[idx] = torch.max(logits)\n",
    "    \n",
    "    logits_l = logits[0:BATCH_SIZE]\n",
    "    logits_w, logits_s = logits[BATCH_SIZE:].chunk(2)\n",
    "\n",
    "    sup_loss = loss_labeled(logits_l, target_x)\n",
    "\n",
    "    psuedo_label = torch.softmax(logits_w.detach_(), dim=-1)\n",
    "    max_probs, psuedo_target = torch.max(psuedo_label, dim=-1)\n",
    "    mask = max_probs.ge(0.95).float()\n",
    "\n",
    "    unsup_loss = (loss_unlabeled(logits_s, psuedo_target)*mask).mean()\n",
    "    \n",
    "    loss = sup_loss + unsup_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
