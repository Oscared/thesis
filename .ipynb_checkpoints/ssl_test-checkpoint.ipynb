{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training function that uses \"FixMatch\" style training with labeled and unlabeled datasets. Configure the original training function from \"test\" to suit the pytorch implementation of FixMatch here: https://github.com/kekmodel/FixMatch-pytorch/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-966ca0403f00>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-966ca0403f00>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def train(net, optimizer, criterion, labeled_data_loader, unlabeled_data_loader epoch, threshold, scheduler=None,\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train(net, optimizer, criterion, labeled_data_loader, unlabeled_data_loader epoch, threshold, scheduler=None,\n",
    "          display_iter=100, device=torch.device('cpu'), display=None,\n",
    "          val_loader=None):\n",
    "    \"\"\"\n",
    "    Training loop to optimize a network for several epochs and a specified loss\n",
    "    Args:\n",
    "        net: a PyTorch model\n",
    "        optimizer: a PyTorch optimizer\n",
    "        labeled_data_loader: a PyTorch dataset loader for the labeled dataset\n",
    "        unlabeled_data_loader: a PyTorch dataset loader for the weakly and strongly augmented, unlabeled dataset\n",
    "        epoch: int specifying the number of training epochs\n",
    "        threshold: probability thresold for pseudo labels acceptance\n",
    "        criterion: a PyTorch-compatible loss function, e.g. nn.CrossEntropyLoss\n",
    "        device (optional): torch device to use (defaults to CPU)\n",
    "        display_iter (optional): number of iterations before refreshing the\n",
    "        display (False/None to switch off).\n",
    "        scheduler (optional): PyTorch scheduler\n",
    "        val_loader (optional): validation dataset\n",
    "        supervision (optional): 'full' or 'semi'\n",
    "    \"\"\"\n",
    "\n",
    "    if criterion is None:\n",
    "        raise Exception(\"Missing criterion. You must specify a loss function.\")\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    save_epoch = epoch // 20 if epoch > 20 else 1\n",
    "\n",
    "\n",
    "    losses = np.zeros(1000000)\n",
    "    mean_losses = np.zeros(100000000)\n",
    "    iter_ = 1\n",
    "    loss_win, val_win = None, None\n",
    "    val_accuracies = []\n",
    "\n",
    "    for e in tqdm(range(1, epoch + 1), desc=\"Training the network\"):\n",
    "        # Set the network to training mode\n",
    "        net.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        train_loader = zip(labeled_data_loader, unlabeled_data_loader)\n",
    "\n",
    "        # Run the training loop for one epoch\n",
    "        for batch_idx, (data_x, data_u) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            # Load the data into the GPU if required\n",
    "            input_x, target_x = data_x\n",
    "            (inputs_u_w, inputs_u_s), _ = data_u\n",
    "            \n",
    "            batch_size = inputs_x.shape[0]\n",
    "            \n",
    "            inputs = torch.cat((inputs_x, inputs_u_w, inputs_u_s)).to(args.device)\n",
    "            targets_x = targets_x.to(device)\n",
    "            logits = net(inputs)\n",
    "            logits_x = logits[:batch_size]\n",
    "            logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "            del logits\n",
    "\n",
    "            Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "\n",
    "            pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
    "            max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "            mask = max_probs.ge(args.threshold).float()\n",
    "\n",
    "            Lu = (F.cross_entropy(logits_u_s, targets_u,\n",
    "                              reduction='none') * mask).mean()\n",
    "\n",
    "            loss = Lx + args.lambda_u * Lu\n",
    "    \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            losses[iter_] = loss.item()\n",
    "            mean_losses[iter_] = np.mean(losses[max(0, iter_ - 100):iter_ + 1])\n",
    "\n",
    "            if display_iter and iter_ % display_iter == 0:\n",
    "                string = 'Train (epoch {}/{}) [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
    "                string = string.format(e, epoch, batch_idx * len(data), len(data) * len(data_loader),\n",
    "                                       100. * batch_idx / len(data_loader), mean_losses[iter_])\n",
    "                update = None if loss_win is None else 'append'\n",
    "                loss_win = display.line(\n",
    "                    X=np.arange(iter_ - display_iter, iter_),\n",
    "                    Y=mean_losses[iter_ - display_iter:iter_],\n",
    "                    win=loss_win,\n",
    "                    update=update,\n",
    "                    opts={'title': \"Training loss\",\n",
    "                          'xlabel': \"Iterations\",\n",
    "                          'ylabel': \"Loss\"\n",
    "                         }\n",
    "                )\n",
    "                tqdm.write(string)\n",
    "\n",
    "                if len(val_accuracies) > 0:\n",
    "                    val_win = display.line(Y=np.array(val_accuracies),\n",
    "                                           X=np.arange(len(val_accuracies)),\n",
    "                                           win=val_win,\n",
    "                                           opts={'title': \"Validation accuracy\",\n",
    "                                                 'xlabel': \"Epochs\",\n",
    "                                                 'ylabel': \"Accuracy\"\n",
    "                                                })\n",
    "            iter_ += 1\n",
    "            del(data_x, data_u, loss)\n",
    "\n",
    "        # Update the scheduler\n",
    "        avg_loss /= len(data_loader)\n",
    "        if val_loader is not None:\n",
    "            val_acc = val(net, val_loader, device=device, supervision=supervision)\n",
    "            val_accuracies.append(val_acc)\n",
    "            metric = -val_acc\n",
    "        else:\n",
    "            metric = avg_loss\n",
    "\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(metric)\n",
    "        elif scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Save the weights\n",
    "        if e % save_epoch == 0:\n",
    "            save_model(net, utils.camel_to_snake(str(net.__class__.__name__)), \n",
    "                       data_loader.dataset.name, epoch=e, metric=abs(metric))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a cosine training scheme with warm-up. Shown to be good, taken from the same resource as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / \\\n",
    "            float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for data augmentation for mixup according to implementation at https://github.com/facebookresearch/mixup-cifar10/blob/master/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=False):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training function that implements mixup augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
