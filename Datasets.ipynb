{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file contains the PyTorch dataset for hyperspectral images and related helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages needed to unpack the datasets from .mat files and convert to PyTorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spectral\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "try:\n",
    "    # Python 3\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:\n",
    "    # Python 2\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "from utils import open_file\n",
    "\n",
    "dataset_path = \"/home/oscar/Desktop/Exjobb/Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to show progress bar when loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmUpTo(tqdm):\n",
    "    \"\"\"Provides `update_to(n)` which uses `tqdm.update(delta_n)`.\"\"\"\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        \"\"\"\n",
    "        b  : int, optional\n",
    "            Number of blocks transferred so far [default: 1].\n",
    "        bsize  : int, optional\n",
    "            Size of each block (in tqdm units) [default: 1].\n",
    "        tsize  : int, optional\n",
    "            Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "        \"\"\"\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)  # will also set self.n = b * bsize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get a dataset and return the related components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name, target_folder=\"/home/oscar/Desktop/Exjobb/Data/\"):\n",
    "    \"\"\" Gets the dataset specified by name and return the related components.\n",
    "    Args:\n",
    "        dataset_name: string with the name of the dataset\n",
    "        target_folder (optional): folder to store the datasets, defaults to ./\n",
    "        datasets (optional): dataset configuration dictionary, defaults to prebuilt one\n",
    "    Returns:\n",
    "        img: 3D hyperspectral image (WxHxB)\n",
    "        gt: 2D int array of labels\n",
    "        label_values: list of class names\n",
    "        ignored_labels: list of int classes to ignore\n",
    "        rgb_bands: int tuple that correspond to red, green and blue bands\n",
    "    \"\"\"\n",
    "    palette = None\n",
    "    \n",
    "    folder = target_folder \n",
    "    \n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    if dataset_name == 'PaviaU':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'PaviaU.mat')['paviaU']\n",
    "\n",
    "        rgb_bands = (55, 41, 12)\n",
    "\n",
    "        gt = open_file(folder + 'PaviaU_gt.mat')['paviaU_gt']\n",
    "\n",
    "        label_values = ['Undefined', 'Asphalt', 'Meadows', 'Gravel', 'Trees',\n",
    "                        'Painted metal sheets', 'Bare Soil', 'Bitumen',\n",
    "                        'Self-Blocking Bricks', 'Shadows']\n",
    "\n",
    "        ignored_labels = [0]\n",
    "\n",
    "    elif dataset_name == 'Salinas':\n",
    "        # Load the image\n",
    "        img = open_file(folder + 'Salinas_corrected.mat')\n",
    "        img = img['salinas_corrected']\n",
    "\n",
    "        rgb_bands = (43, 21, 11)  # AVIRIS sensor\n",
    "\n",
    "        gt = open_file(folder + 'Salinas_gt.mat')['salinas_gt']\n",
    "        label_values = [\"Undefined\", \"Brocoli_green_weeds_1\", \"Brocoli_green_weeds_2\", \"Fallow\",\n",
    "                        \"Fallow_rough_plow\", \"Fallow_smooth\", \"Stubble\",\n",
    "                        \"Celery\", \"Grapes_untrained\", \"Soil_vinyard_develop\",\n",
    "                        \"Corn_senesced_green_weeds\", \"Lettuce_romaine_4wk\", \"Lettuce_romaine_5wk\",\n",
    "                        \"Lettuce_romaine_6wk\", \"Lettuce_romaine_7wk\", \"Vinyard_untrained\",\n",
    "                        \"Vinyard_vertical_trellis\"]\n",
    "\n",
    "        ignored_labels = [0]\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print(\"Error: no dataset of the requested type found. Available datasets are PaviaU, Salinas.\")\n",
    "        \n",
    "    # Filter NaN out\n",
    "    nan_mask = np.isnan(img.sum(axis=-1))\n",
    "    if np.count_nonzero(nan_mask) > 0:\n",
    "       print(\"Warning: NaN have been found in the data. It is preferable to remove them beforehand. Learning on NaN data is disabled.\")\n",
    "    img[nan_mask] = 0\n",
    "    gt[nan_mask] = 0\n",
    "    ignored_labels.append(0)\n",
    "\n",
    "    ignored_labels = list(set(ignored_labels))\n",
    "    # Normalization\n",
    "    img = np.asarray(img, dtype='float32')\n",
    "    #img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "    data = img.reshape(np.prod(img.shape[:2]), np.prod(img.shape[2:]))\n",
    "    #data = preprocessing.scale(data)\n",
    "    data  = preprocessing.minmax_scale(data)\n",
    "    img = data.reshape(img.shape)\n",
    "    \n",
    "    return img, gt, label_values, ignored_labels, rgb_bands, palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Hyperspectral scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperX(torch.utils.data.Dataset):\n",
    "    \"\"\" Generic class for a hyperspectral scene \"\"\"\n",
    "\n",
    "    def __init__(self, data, gt, **hyperparams):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: 3D hyperspectral image\n",
    "            gt: 2D array of labels\n",
    "            patch_size: int, size of the spatial neighbourhood\n",
    "            center_pixel: bool, set to True to consider only the label of the\n",
    "                          center pixel\n",
    "            data_augmentation: bool, set to True to perform random flips\n",
    "            supervision: 'full' or 'semi' supervised algorithms\n",
    "        \"\"\"\n",
    "        super(HyperX, self).__init__()\n",
    "        self.data = data\n",
    "        self.label = gt\n",
    "        self.patch_size = hyperparams['patch_size']\n",
    "        self.name = hyperparams['dataset']\n",
    "        self.ignored_labels = set(hyperparams['ignored_labels'])\n",
    "        self.flip_augmentation = hyperparams['flip_augmentation']\n",
    "        self.radiation_augmentation = hyperparams['radiation_augmentation'] \n",
    "        self.mixture_augmentation = hyperparams['mixture_augmentation'] \n",
    "        self.center_pixel = hyperparams['center_pixel']\n",
    "        supervision = hyperparams['supervision']\n",
    "        # Fully supervised : use all pixels with label not ignored\n",
    "        if supervision == 'full':\n",
    "            mask = np.ones_like(gt)\n",
    "            for l in self.ignored_labels:\n",
    "                mask[gt == l] = 0\n",
    "        # Semi-supervised : use all pixels, except padding\n",
    "        elif supervision == 'semi':\n",
    "            mask = np.ones_like(gt)\n",
    "        x_pos, y_pos = np.nonzero(mask)\n",
    "        p = self.patch_size // 2\n",
    "        self.indices = np.array([(x,y) for x,y in zip(x_pos, y_pos) if x > p and x < data.shape[0] - p and y > p and y < data.shape[1] - p])\n",
    "        self.labels = [self.label[x,y] for x,y in self.indices]\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    @staticmethod\n",
    "    def flip(*arrays):\n",
    "        horizontal = np.random.random() > 0.5\n",
    "        vertical = np.random.random() > 0.5\n",
    "        if horizontal:\n",
    "            arrays = [np.fliplr(arr) for arr in arrays]\n",
    "        if vertical:\n",
    "            arrays = [np.flipud(arr) for arr in arrays]\n",
    "        return arrays\n",
    "\n",
    "    @staticmethod\n",
    "    def radiation_noise(data, alpha_range=(0.9, 1.1), beta=1/25):\n",
    "        alpha = np.random.uniform(*alpha_range)\n",
    "        noise = np.random.normal(loc=0., scale=1.0, size=data.shape)\n",
    "        return alpha * data + beta * noise\n",
    "\n",
    "    def mixture_noise(self, data, label, beta=1/25):\n",
    "        alpha1, alpha2 = np.random.uniform(0.01, 1., size=2)\n",
    "        noise = np.random.normal(loc=0., scale=1.0, size=data.shape)\n",
    "        data2 = np.zeros_like(data)\n",
    "        for  idx, value in np.ndenumerate(label):\n",
    "            if value not in self.ignored_labels:\n",
    "                l_indices = np.nonzero(self.labels == value)[0]\n",
    "                l_indice = np.random.choice(l_indices)\n",
    "                assert(self.labels[l_indice] == value)\n",
    "                x, y = self.indices[l_indice]\n",
    "                data2[idx] = self.data[x,y]\n",
    "        return (alpha1 * data + alpha2 * data2) / (alpha1 + alpha2) + beta * noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.indices[i]\n",
    "        x1, y1 = x - self.patch_size // 2, y - self.patch_size // 2\n",
    "        x2, y2 = x1 + self.patch_size, y1 + self.patch_size\n",
    "\n",
    "        data = self.data[x1:x2, y1:y2]\n",
    "        label = self.label[x1:x2, y1:y2]\n",
    "\n",
    "        if self.flip_augmentation and self.patch_size > 1:\n",
    "            # Perform data augmentation (only on 2D patches)\n",
    "            data, label = self.flip(data, label)\n",
    "        if self.radiation_augmentation and np.random.random() < 0.1:\n",
    "                data = self.radiation_noise(data)\n",
    "        if self.mixture_augmentation and np.random.random() < 0.2:\n",
    "                data = self.mixture_noise(data, label)\n",
    "\n",
    "        # Copy the data into numpy arrays (PyTorch doesn't like numpy views)\n",
    "        data = np.asarray(np.copy(data).transpose((2, 0, 1)), dtype='float32')\n",
    "        label = np.asarray(np.copy(label), dtype='int64')\n",
    "\n",
    "        # Load the data into PyTorch tensors\n",
    "        data = torch.from_numpy(data)\n",
    "        label = torch.from_numpy(label)\n",
    "        # Extract the center label if needed\n",
    "        if self.center_pixel and self.patch_size > 1:\n",
    "            label = label[self.patch_size // 2, self.patch_size // 2]\n",
    "        # Remove unused dimensions when we work with invidual spectrums\n",
    "        elif self.patch_size == 1:\n",
    "            data = data[:, 0, 0]\n",
    "            label = label[0, 0]\n",
    "\n",
    "        # Add a fourth dimension for 3D CNN\n",
    "        if self.patch_size > 1:\n",
    "            # Make 4D data ((Batch x) Planes x Channels x Width x Height)\n",
    "            data = data.unsqueeze(0)\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
